{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "import numpy as np\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# import matplotlib.pyplot as plt\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('Dataset5_raw_train.npz')\n",
    "image_train, image_label_train = data_train['image'].astype(np.float16) / 255.0, data_train['image_label'].astype(\n",
    "    np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([image_train[:2326], image_train[2512:4838], image_train[6119:8445], image_train[12234:14560],\n",
    "                          image_train[16672:18998]])\n",
    "y_train = np.concatenate([image_label_train[:2326], image_label_train[2512:4838], image_label_train[6119:8445],\n",
    "                          image_label_train[12234:14560], image_label_train[16672:18998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = np.load('Dataset5_raw_val.npz')\n",
    "image_val, image_label_val = data_val['image'].astype(np.float16) / 255.0, data_val['image_label'].astype(np.int8)\n",
    "\n",
    "x_val = np.concatenate(\n",
    "    [image_val[:836], image_val[837:1673], image_val[2040:2876], image_val[4079:4915], image_val[5559:6395]])\n",
    "y_val = np.concatenate(\n",
    "    [image_label_val[:836], image_label_val[837:1673], image_label_val[2040:2876], image_label_val[4079:4915],\n",
    "     image_label_val[5559:6395]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 81s 101ms/step - loss: 3.1415 - accuracy: 0.5971 - val_loss: 3.6438 - val_accuracy: 0.1957\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.4913 - accuracy: 0.8370 - val_loss: 2.2538 - val_accuracy: 0.4216\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 25s 67ms/step - loss: 0.3616 - accuracy: 0.8727 - val_loss: 0.5239 - val_accuracy: 0.8091\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.3310 - accuracy: 0.8887 - val_loss: 0.3436 - val_accuracy: 0.8909\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.2727 - accuracy: 0.9081 - val_loss: 0.4823 - val_accuracy: 0.8611\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.2648 - accuracy: 0.9136 - val_loss: 0.3018 - val_accuracy: 0.8938\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.2660 - accuracy: 0.9096 - val_loss: 1.2238 - val_accuracy: 0.6284\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.2600 - accuracy: 0.9127 - val_loss: 0.2523 - val_accuracy: 0.9072\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.2058 - accuracy: 0.9327 - val_loss: 0.5086 - val_accuracy: 0.8582\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 24s 68ms/step - loss: 0.2136 - accuracy: 0.9270 - val_loss: 0.5856 - val_accuracy: 0.8010\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 25s 68ms/step - loss: 0.2026 - accuracy: 0.9276 - val_loss: 0.9031 - val_accuracy: 0.7668\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.2055 - accuracy: 0.9291 - val_loss: 0.4606 - val_accuracy: 0.8442\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.1850 - accuracy: 0.9380 - val_loss: 0.2988 - val_accuracy: 0.8913\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 25s 68ms/step - loss: 0.1806 - accuracy: 0.9378 - val_loss: 1.2518 - val_accuracy: 0.6721\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.1507 - accuracy: 0.9507 - val_loss: 2.4332 - val_accuracy: 0.4837\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 25s 68ms/step - loss: 0.1974 - accuracy: 0.9364 - val_loss: 1.0311 - val_accuracy: 0.6875\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.1153 - accuracy: 0.9593 - val_loss: 0.3180 - val_accuracy: 0.8957\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.1492 - accuracy: 0.9511 - val_loss: 0.9997 - val_accuracy: 0.7582\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 24s 67ms/step - loss: 0.1090 - accuracy: 0.9647 - val_loss: 0.6475 - val_accuracy: 0.8115\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 25s 69ms/step - loss: 0.1280 - accuracy: 0.9579 - val_loss: 0.4454 - val_accuracy: 0.8606\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (224, 224, 3)\n",
    "# Pre-trained model with MobileNetV2\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "# Freeze the pre-trained model weights\n",
    "base_model.trainable = True\n",
    "\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Trainable classification head\n",
    "maxpool_layer = GlobalMaxPooling2D()\n",
    "flatten = Flatten()\n",
    "dense1 = Dense(1024, activation='relu')\n",
    "prediction_layer = Dense(units=5, activation='softmax')\n",
    "# Layer classification head with feature detector\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    maxpool_layer, flatten, dense1,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "trainX = x_train\n",
    "testX = x_val\n",
    "trainY = y_train\n",
    "testY = y_val\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "H = model.fit(trainX, trainY, batch_size=16, steps_per_epoch=len(trainX) // 32, validation_data=(testX, testY),\n",
    "              validation_steps=len(testX) // 32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "894b3b24c145c270a8b7351330716103ae0f8142438128cd5931ba0aa38e4d56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
